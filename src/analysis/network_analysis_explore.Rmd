---
title: "Exploratory analysis of emotion similarity networks"
author: "Ashish"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    theme: paper
---

In this document, I am conducting exploratory analyses of the recently developed emotion similarity network task. In the task, participants select which two of a set of three emotional images are most similar in their emotional quality. I hypothesize that participants who exhibit self-report characteristics of greater emotional awareness will have more distinct communities in a within person network analysis of each participants' similarity structure.

# Initialization

```{r, message=FALSE}
rm(list=ls())
library(lmerTest)
library(igraph)
library(kableExtra)
library(here)
library(tidyverse)
theme_set(theme_bw())
```

```{r, eval=FALSE}
library(tidylog)
```

# Preprocessing

## Functions

```{r}
#' scale_numerics
#' 
#' z-scores values of all numeric columns
#' 
#' @param df input dataframe
#' @param new_cols boolean indicating whether to create new columns when scaling or scale in place
#' @param ... arguments to be passed to rsurveyutils::scale()
#'
#' @return dataframe with scaled values
scale_numerics <- function(df, new_cols=TRUE, ...){
  
  if(new_cols){df %>% mutate_if(is.numeric, list(sc = ~rsurveyutils::scale(., ...)))}
  else {df %>% mutate_if(is.numeric,  ~scale(., ...))}
}

```


## Read & process survey data
```{r, message=FALSE}
survey_filepath <- "data/proc/emo_similarity_network_presurvey_sum2021_proc.csv"
df_survey_raw <- read_csv(here(survey_filepath))
df_survey <- df_survey_raw %>% 
  group_by(pid) %>%
  # -- remove failed attention checks -- #
  filter((all(ac_paq_pass == 1) & all(ac_pmerq_pass == 1) & all(ac_tas_pass == 1))) %>% 
  ungroup
```

## Read & process task data
```{r, message=FALSE}
task_filepath <- "data/proc/emoSimilarityTask_proc.csv"

df_task_raw <- read_csv(here(task_filepath))

df_task <- df_task_raw
```

## Create data frame of forced choice attention checks

Currently using a .33 minimum threshold of passing attention checks

.33 = 4 correct out of 12, 15% chance of passing threshold if you're guessing

```{r}
ac_minimum_threshold <- .33

df_ac_forceChoice <- df_task %>% 
  drop_na(ac_forceChoice) %>% 
  group_by(pid, pool) %>% 
  summarize(pr_correct = mean(ac_forceChoice)) %>% 
  ungroup
df_ac_forceChoice %>% 
  ggplot(aes(x = pr_correct)) + 
  geom_histogram()
ac_fail_pids <- df_ac_forceChoice %>% 
  filter(pr_correct < ac_minimum_threshold) %>% 
  pull(pid)

# df_ac_forceChoice %>% 
#   filter(pool == "rep" & pr_correct < .33)
  
```

## Create data frame of task trials

```{r}
df_trials <- df_task %>% 
  # -- keep only trials -- #
  filter(trial_type == "image-multi-select") %>%
  filter(!str_detect(img_file_1, "practice_images")) %>%
  # -- remove AC fails -- #
  filter(!pid %in% ac_fail_pids) %>% 
  # -- preproc -- #
  select(pid, trial_index, trial_valence,
         img_file_chosen_1, img_file_chosen_2,
         img_file_1, img_file_2, img_file_3) %>% 
  pivot_longer(c(img_file_1, img_file_2, img_file_3), 
               values_to="img_file", names_to="label") %>% 
  mutate(is_chosen=ifelse(img_file == img_file_chosen_1 | 
                            img_file == img_file_chosen_2, 
                          1, 0)) %>% 
  select(-c(img_file_chosen_1, img_file_chosen_2)) %>% 
  arrange(pid, trial_index, img_file)

df_trials_wide <- df_trials %>% 
  arrange(pid, trial_index, desc(is_chosen), img_file) %>% 
  select(-label) %>% 
  group_by(pid, trial_index) %>% 
  mutate(img_num = 1:n()) %>% 
  ungroup %>% 
  pivot_wider(id_cols = c(pid, trial_index, trial_valence), names_from = img_num, values_from=c(img_file, is_chosen))
```

# Create graphs community structure and fit clustering

Create clusters using walktrap algorithm

[A Comparative Analysis of Community Detection Algorithms on Artificial Networks](https://www.nature.com/articles/srep30750.pdf)
```{r}
get_graph <- function(df){
   df %>%
    select(img_file_1, img_file_2) %>% 
    graph_from_data_frame(directed=FALSE) 
}

df_graphs <- df_trials_wide %>% 
  group_by(pid, trial_valence) %>% 
  nest() %>%
  mutate(graph = map(data, get_graph)) %>% 
  mutate(cluster = map(graph, cluster_walktrap)) %>% 
  mutate(modularity = map_dbl(cluster, modularity)) %>% 
  # mutate(cluster_plot = map2(cluster, graph, ~plot(.x, .y))) %>% 
  mutate(n_clusters = map_dbl(cluster, ~length(unique(.$membership))))
```

```{r, echo=FALSE}
# ------ End Preprocessing ------ #
# ----- Run all chunks above -----#
```

# Sample characteristics

## Number of rows per participant

```{r}
df_task %>% 
  group_by(pid) %>% 
  summarise(n = n()) %>% 
  arrange(n) %>% 
  kbl %>% 
  kable_styling %>% 
  scroll_box(height="500px", width = "300px")

df_task %>% pull(pid) %>% unique
```

## Response time distribution

```{r}
# participant mean response time
df_task %>% 
  mutate(rt = as.numeric(rt)/1000) %>% 
  filter(rsurveyutils::scale(rt) < 3) %>% # remove outliers if 3 SDs above mean
  group_by(pid) %>% 
  summarize(mean_rt = mean(rt)) %>% 
  ggplot(aes(x = mean_rt)) + 
  geom_histogram()

# trial response time
df_task %>% 
  mutate(rt = as.numeric(rt)/1000) %>% 
  filter(rsurveyutils::scale(rt) < 3) %>% # remove if 3 SDs above mean
  ggplot(aes(x = rt)) + 
  geom_histogram()

```

## Plot total time spent on survey

```{r}
df_task %>% 
  group_by(run_id) %>% 
  filter(time_elapsed == max(time_elapsed)) %>% 
  ungroup %>% 
  mutate(mins_elapsed = as.numeric(time_elapsed)/(1000*60)) %>% 
  ggplot(aes(x = mins_elapsed)) + 
  geom_histogram()

```

# Plot all participant networks and clusters and export to PDF

I'm also including their PAQ score and their responses to the attention checks in the within person network PDFs.

```{r eval=FALSE}
clusterPlots_dir <- here(paste0("results/", Sys.Date(), "/clusterPlots"))
dir.create(clusterPlots_dir, recursive=TRUE)
pdf(paste0(clusterPlots_dir, "/pid_valence_clusterPlots.pdf"))
for(i in 1:nrow(df_graphs)){
  # plot of clusters
  plot(df_graphs$cluster[[i]],
       df_graphs$graph[[i]],
       main=paste(df_graphs$pid[i], df_graphs$trial_valence[i]))
  # get free response dimensions of clustering
  freeResp <- df_task %>%
    filter(pid == df_graphs$pid[[i]]) %>%
    filter(ac_freeResp != "") %>%
    filter(trial_valence == df_graphs$trial_valence[[i]]) %>%
    pull(ac_freeResp)
  if(length(freeResp) > 0){
    text(0,-1.4, labels=paste(freeResp, collapse = "\n"))
  }
  # get attention check score
  pr_correct <- df_ac_forceChoice %>%
    filter(pid == df_graphs$pid[[i]]) %>%
    pull(pr_correct)
  text(-1.2,1.3, labels=paste("AC correct pr:", round(pr_correct, 2)))
  # get PAQ score
  paq_score <- df_survey %>%
    mutate(paq = rsurveyutils::scale(paq)) %>%
    filter(pid == df_graphs$pid[[i]]) %>%
    pull(paq)
    text(-1.2,seq(1.1, by = -.1, length.out = length(paq_score)),
         labels=paste("PAQ:", round(paq_score, 2)))

}
dev.off()
```

# Differences by trial valence

Here I'm exploring whether network metrics are different depending on what valence type of block the participant was in

## Modularity metric per trial valence

[Modularity metric](https://en.wikipedia.org/wiki/Modularity_(networks))

```{r}
df_graphs %>% 
  ggplot(aes(y = modularity, x = trial_valence)) + 
  # geom_point(alpha = .3, position=position_jitter(.1)) +
  stat_summary(fun.data = "mean_cl_boot") 

df_graphs %>% 
  lmer(modularity ~ (1|pid), .) %>% 
  summary
df_graphs %>% 
  lmer(modularity ~ trial_valence + (1|pid), .) %>% 
  summary
```


## Mean clusters per trial valence

```{r}
df_graphs %>% 
  ggplot(aes(y = n_clusters, x = trial_valence)) + 
  # geom_point(alpha = .3, position=position_jitter(.1)) + 
  stat_summary(fun.data = "mean_cl_boot") 

df_graphs %>% 
  lmer(n_clusters ~ (1|pid), .) %>% 
  summary
df_graphs %>% 
  lmer(n_clusters ~ trial_valence + (1|pid), .) %>% 
  summary

```

# Relationship between modularity metric and number of clusters metric

```{r}
cor.test(df_graphs$modularity, df_graphs$n_clusters)
```

# Relation of modularity to alexithymia

Here and in the valence analysis, the modularity metric is not behaving as expected. I have not continued with modularity analyses beyond the full scale of alexithymia for this reason.

```{r}
dat <- df_survey %>% 
  select(pid, paq) %>% 
  inner_join(df_graphs)

dat %>% 
  ggplot(aes(x = paq, y = modularity, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")

# dat %>% 
#   lmer(modularity ~ paq + (1|pid), .) %>% 
#   summary
```

# Relation of n clusters to variables

## Alexithymia

```{r}
dat <- df_survey %>% 
  select(pid, matches("paq(.*[^0-9]$|$)")) %>% 
  inner_join(df_graphs)

## Full scale
dat %>% 
  ggplot(aes(x = paq, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")
dat %>% 
  filter(trial_valence == "negative") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq, .) %>% 
  summary
dat %>% 
  filter(trial_valence == "neutral") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq, .) %>% 
  summary
dat %>% 
  filter(trial_valence == "positive") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq, .) %>% 
  summary

## DIF

dat %>% 
  ggplot(aes(x = paq_dif, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")

# DDF
dat %>% 
  ggplot(aes(x = paq_ddf, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")

## EOT
dat %>% 
  ggplot(aes(x = paq_eot, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")

## PAQ Pos
dat %>% 
  ggplot(aes(x = paq_pos, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")
dat %>% 
  filter(trial_valence == "negative") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq_pos, .) %>% 
  summary
dat %>% 
  filter(trial_valence == "neutral") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq_pos, .) %>% 
  summary
dat %>% 
  filter(trial_valence == "positive") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq_pos, .) %>% 
  summary
  

## PAQ Neg 
dat %>% 
  ggplot(aes(x = paq_neg, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")
dat %>% 
  filter(trial_valence == "negative") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq_neg, .) %>% 
  summary
dat %>% 
  filter(trial_valence == "neutral") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq_neg, .) %>% 
  summary
dat %>% 
  filter(trial_valence == "positive") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ paq_neg, .) %>% 
  summary
  


```

## ERQ Reappraisal
```{r}
dat <- df_survey %>% 
  select(pid, matches("^erq(.*[^0-9]$|$)")) %>% 
  inner_join(df_graphs)

dat %>% 
  ggplot(aes(x = erq_reap_fr, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")

dat %>% 
  filter(trial_valence == "negative" ) %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ erq_reap_fr, . ) %>% 
  summary

dat %>% 
  filter(trial_valence == "negative" | trial_valence == "neutral") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ erq_reap_fr*trial_valence, . ) %>% 
  summary

```

## ERQ suppression
```{r}
dat <- df_survey %>% 
  select(pid, matches("^erq(.*[^0-9]$|$)")) %>% 
  inner_join(df_graphs)

dat %>% 
  ggplot(aes(x = erq_supp_fr, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3) + 
  geom_smooth(method="lm")

dat %>% 
  filter(trial_valence == "negative" ) %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ erq_supp_fr, . ) %>% 
  summary

dat %>% 
  filter(trial_valence == "negative" | trial_valence == "neutral") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ erq_supp_fr*trial_valence, . ) %>% 
  summary

```

## PHQ
```{r}
dat <- df_survey %>% 
  select(pid, matches("^phq(.*[^0-9]$|$)"), -matches("timer")) %>% 
  inner_join(df_graphs)

dat %>% 
  ggplot(aes(x = phq, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3, position = position_dodge(.6)) + 
  geom_smooth(method="lm")
dat %>% 
  filter(trial_valence == "negative" ) %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ phq, . ) %>% 
  summary

dat %>% 
  filter(trial_valence == "negative" | trial_valence == "neutral") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ phq*trial_valence, . ) %>% 
  summary

```

## GAD
```{r}
dat <- df_survey %>% 
  select(pid, matches("^gad(.*[^0-9]$|$)"), -matches("timer")) %>% 
  inner_join(df_graphs)

dat %>% 
  ggplot(aes(x = gad, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3, position = position_dodge(.6)) + 
  geom_smooth(method="lm")
dat %>% 
  filter(trial_valence == "negative" ) %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ gad, . ) %>% 
  summary

dat %>% 
  filter(trial_valence == "negative" | trial_valence == "neutral") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ gad*trial_valence, . ) %>% 
  summary

```

## Fatigue
```{r}
dat <- df_survey %>% 
  select(pid, matches("^fss(.*[^0-9]$|$)"), -matches("timer")) %>% 
  inner_join(df_graphs)

dat %>% 
  ggplot(aes(x = fss, y = n_clusters, color = trial_valence)) + 
  geom_point(alpha = .3, position = position_dodge(.6)) + 
  geom_smooth(method="lm")
dat %>% 
  filter(trial_valence == "negative" ) %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ fss, . ) %>% 
  summary
dat %>% 
  filter(trial_valence == "positive" ) %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ fss, . ) %>% 
  summary

dat %>% 
  filter(trial_valence == "negative" | trial_valence == "neutral") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(n_clusters ~ fss*trial_valence, . ) %>% 
  summary

```

# Is alx related to psychopathology controlling for n clusters?

* There is not a lot of shared variance it seems. This is surprising since I would think that the schema differences reflected by n_clusters would explain why n_clusters is also related to psychopathology.

```{r}
dat <- df_survey %>% 
  select(pid, matches("^paq(.*[^0-9]$|$)"), phq, gad, -matches("timer")) %>% 
  inner_join(df_graphs)

cor.test(df_survey$paq, df_survey$phq)
cor.test(df_survey$paq_dif, df_survey$phq)
cor.test(df_survey$paq_ddf, df_survey$phq)
cor.test(df_survey$paq_eot, df_survey$phq)
cor.test(df_survey$paq, df_survey$gad)
cor.test(df_survey$paq_dif, df_survey$gad)
cor.test(df_survey$paq_ddf, df_survey$gad)
cor.test(df_survey$paq_eot, df_survey$gad)

dat %>% 
  filter(trial_valence == "negative") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(phq ~ n_clusters, .) %>% 
  summary
dat %>% 
  filter(trial_valence == "negative") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(phq ~ paq, .) %>% 
  summary
dat %>% 
  filter(trial_valence == "negative") %>% 
  mutate_if(is.numeric, scale) %>% 
  lm(phq ~ n_clusters + paq, .) %>% 
  summary

```

# Explanations of why they chose what they did

TODO
```{r}

```

# More TODOs

* Explore differences in degree related metrics 
  * mean degree per person 
  * can I do some kind of clustering of degree distributions to identify classsification schemas? 
    * [Degree distribution wikipedia](https://en.wikipedia.org/wiki/Degree_distribution)
    * [clustering pr distributions](https://stats.stackexchange.com/questions/13186/clustering-probability-distributions-methods-metrics) 

# Session Info

```{r}
sessionInfo()
```
